{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aS4GZ37Wgcjr"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 2, Module 2*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etFf1WLWgcjt",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Train (Prepare)\n",
    "__*Neural Network Foundations*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXB80QOhgcju"
   },
   "source": [
    "## Learning Objectives\n",
    "* <a href=\"#p1\">Part 1</a>: Student should be able to explain the intuition behind backpropagation and gradient descent\n",
    "* <a href=\"#p2\">Part 2</a>: Student should be able to discuss the importance of batch size\n",
    "* <a href=\"#p3\">Part 3</a>: Student should be able to discuss the importance of learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YuQu2lfgcju"
   },
   "source": [
    "## Summary of Yesterday\n",
    "\n",
    "Yesterday, we learned about some of the principal components of Neural Networks: Neurons, Weights, Activation Functions, and layers (input, output, & hidden). Today, we will reinforce our understanding of those components and introduce the mechanics of training a neural network. Feed-forward neural networks, such as multi-layer perceptrons (MLPs), are almost always trained using some variation of gradient descent where the gradient has been calculated by backpropagation.\n",
    "\n",
    "  <center><img src=\"https://raw.githubusercontent.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/main/module1-Architect/IMG_0167.jpeg\" width=400></center>\n",
    "\n",
    "- There are three kinds of layers: input, hidden, and output layers.\n",
    "- Each layer is made up of **n** individual neurons (aka activation units) which have a corresponding weight and bias.\n",
    "- Signal is passed from layer to layer through a network by:\n",
    " - Taking in inputs from the training data (or previous layer)\n",
    " - Multiplying each input by its corresponding weight (think arrow/connecting line)\n",
    " - Adding a bias to this weighted some of inputs and weights\n",
    " - Activating this weighted sum + bias by squishifying it with sigmoid or some other activation function. With a single perceptron with three inputs, calculating the output from the node is done like so:\n",
    "\\begin{align}\n",
    " y = sigmoid(\\sum(weight_{1}input_{1} + weight_{2}input_{2} + weight_{3}input_{3}) + bias)\n",
    "\\end{align}\n",
    " - this final activated value is the signal that gets passed onto the next layer of the network.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpi4R03rgcjv"
   },
   "source": [
    "## Training a Neural Network: *Formal Summary*\n",
    "\n",
    "0. Pick a network architecture\n",
    "   - No. of input units = No. of features\n",
    "   - No. of output units = Number of Classes (or expected targets)\n",
    "   - Select the number of hidden layers and number of neurons within each hidden layer\n",
    "1. Randomly initialize weights\n",
    "2. Implement forward propagation to get $h_{\\theta}(x^{(i)})$ for any $x^{(i)}$\n",
    "3. Implement code to compute a cost function $J(\\theta)$\n",
    "4. Implement backpropagation to compute partial derivatives $\\frac{\\delta}{\\delta\\theta_{jk}^{l}}{J(\\theta)}$\n",
    "5. Use gradient descent (or other advanced optimizer) with backpropagation to minimize $J(\\theta)$ as a function of parameters $\\theta\\$\n",
    "6. Repeat steps 2 - 5 until cost function is 'minimized' or some other stopping criteria is met. One pass over steps 2 - 5 is called an iteration or epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aM4CK1IarId4",
    "toc-hr-collapsed": false
   },
   "source": [
    "------\n",
    "# Backpropagation & Gradient Descent (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ktm8Fmoagcjy",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Overview\n",
    "\n",
    "Backpropagation is short for [\"Backwards Propagation of errors\"](https://en.wikipedia.org/wiki/Backpropagation) and refers to a specific (rather calculus intensive) algorithm for how weights in a neural network are updated in reverse order at the end of each training epoch. Our purpose today is to demonstrate the backpropagation algorithm on a simple Feedforward Neural Network and in so doing help you get a grasp on the main process. If you want to understand all of the underlying calculus of how the gradients are calculated then you'll need to dive into it yourself, [3Blue1Brown's video is a great starting place](https://www.youtube.com/watch?v=tIeHLnjs5U8). I also highly recommend this Welch Labs series [Neural Networks Demystified](https://www.youtube.com/watch?v=bxe2T-V8XRs) if you want a rapid yet orderly walk through of the main intuitions and math behind the backpropagation algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXI2tEO9gcjy"
   },
   "source": [
    "### What is a Gradient?\n",
    "\n",
    "> In vector calculus, the gradient is a multi-variable generalization of the derivative. \n",
    "\n",
    "The gradients that we will deal with today will be vector representations of the derivative of the activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: A basic derivative \n",
    "\n",
    "![](https://ginsyblog.files.wordpress.com/2017/02/derivativelimitdef.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients: Multi-dimensional derivatives\n",
    "\n",
    "![](data:image/png;base64,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)\n",
    "\n",
    "\n",
    "\n",
    "Because a derivative can have a component in multiple dimensions, we define a gradient as a multi-dimensional derivative that takes on the form of a vector. Why a vector? Because gradients have both direction and magnitude. \n",
    "\n",
    "**In short, gradients point in the direction of greatest change.**\n",
    "![](https://i.stack.imgur.com/OI6Gy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Formula \n",
    "This is a general formula for standard Gradient Descent. \n",
    "![](https://media.geeksforgeeks.org/wp-content/uploads/20200611183120/1406-7.png)\n",
    "\n",
    "There are more sophisticated versions of gradient descent commonly referred to as [**Adaptive Gradient Descent**](https://ruder.io/optimizing-gradient-descent/). \n",
    "\n",
    "\n",
    "These adaptive models all build off of this simple equation, so it's best to first understand the standard form of Gradient Descent, then move on to more sophisticated versions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometry of Gradient Descent \n",
    "\n",
    "![](https://i.stack.imgur.com/yk1mk.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex vs. Non-Convex\n",
    "\n",
    "Loss curves won't always have a single minimum point for gradient descent to converge towards. Sometimes there are multiple minimums. \n",
    "\n",
    "**Global Minimum:** The absolute minimum location of a curve (or surface). \n",
    "\n",
    "**Local Minimum:** The local minimum location of a curve (or surface). \n",
    "\n",
    "### In 2-Dimensions\n",
    "![](https://lh3.googleusercontent.com/o0J1qW2PhvrgsPbYzKgnuNGDyjZF7wug3OBwDPwY5LXD0Vjg3t3otN6ecZ64K8J62sNonpvZxzKTs0pMr9YniDUmQC5J-IFXmSNvRJTbxr9kyAfNP-_A7HdC8hEa9x1dDgnf9jSp)\n",
    "\n",
    "\n",
    "### In 3-Dimensions \n",
    "In Calculus, those ambiguous points that take on the form of both local mins and local maxs are known as [**Saddle points**](https://en.wikipedia.org/wiki/Saddle_point). It's not necessary to dive into the mathematics, the key take away is that non-convex error curves (and surfaces) have this global/local minimum issue. \n",
    "\n",
    "![](https://www.oreilly.com/radar/wp-content/uploads/sites/3/2019/06/convex-non-convex-9c8cb9320d4b0392c5f67004e8832e85.jpg)\n",
    "\n",
    "\n",
    "**Take Away:** The issue is that you might think that gradient descent has converged toward a global minimum but it might actually be stuck in a local minimum. \n",
    "\n",
    "There are at least 2 possible solutions to this problem: \n",
    "\n",
    "1) Use different appraoches to randomly initalizing your model weights\n",
    "For this check out [Keras's docs on Weight Initializers](https://keras.io/api/layers/initializers/). Treat these weight initializers as just another hyper-parameter to include in your gridsearch. It's a good idea to get into the practice of including these in your gridsearches. \n",
    "\n",
    "\n",
    "2) Use non-gradient descent optimizers such as [Particle Swarm](https://en.wikipedia.org/wiki/Particle_swarm_optimization) or [Genetic Algorithms](https://en.wikipedia.org/wiki/Genetic_algorithm). Feel free to read up on these appraoches but know that **you are not expected to know these appraoches** and they are outside the scope of this course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZY66kiUgcjz",
    "toc-hr-collapsed": true
   },
   "source": [
    "------\n",
    "## Follow Along\n",
    "\n",
    "In this section, we will again build a simple neural network using base TensorFlow. We'll focus on using a __Feed Forward Neural Network__ to predict test scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d4tzpwO6B47"
   },
   "source": [
    "### Generate some Fake Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ERyVgeO_IWyV"
   },
   "outputs": [],
   "source": [
    "# ploting \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dataset iimport\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# tensorflow imports for building \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "id": "ERyVgeO_IWyV",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "acd731cd43e78a23018666747114a6ed",
     "grade": false,
     "grade_id": "cell-13ede96854baf6e5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-23 17:43:28.423306: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Imagine that our data is drawn from a linear function\n",
    "\n",
    "# linear regression assumes normally distributed data \n",
    "\n",
    "# y_hat =  x * w  + b  + error/noise\n",
    "\n",
    "# YOUR CODE HERE\n",
    "TRUE_W = 3.5\n",
    "TRUE_b = 50\n",
    "NUM_EXAMPLES = 1000\n",
    "\n",
    "inputs = tf.random.normal(shape=[NUM_EXAMPLES])\n",
    "noise = tf.random.normal(shape=[NUM_EXAMPLES])\n",
    "\n",
    "outputs = inputs * TRUE_W + TRUE_b + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCJesGEUgcj4"
   },
   "source": [
    "### Loss Function\n",
    "Here we will use Mean Squared Error (MSE), because this is a regression problem. We are trying to predict a continuous target.\n",
    "\n",
    "![](https://miro.medium.com/max/808/1*-e1QGatrODWpJkEwqP4Jyg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(target_y, predicted_y, model, reg_strength=0.0):\n",
    "    \"\"\"\n",
    "    Implements Mean Square Error (MSE) as the loss function\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(tf.square(target_y - predicted_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgTf6vTS69Sw"
   },
   "source": [
    "### Neural Network Architecture\n",
    "Lets create a Neural Network class called \"Model\" to contain this functionality. Note: This is essentially a linear regression whose coefficients are trained by gradient descent. In practice, gradient descent works on much more complex function like the multi-layer networks we constructed yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RUI8VSR5zyBv"
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.W = tf.Variable(8.0)\n",
    "        self.b = tf.Variable(40.0)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # returns m*x + b \n",
    "        return self.W * x + self.b\n",
    "\n",
    "model = Model()\n",
    "\n",
    "assert model(3.0).numpy() == 64.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbyT_FJ88IlK"
   },
   "source": [
    "### Initial Weights\n",
    "The initial weights in our model were arbitrary. In practice, weights are initialized randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "IreIDe6P8H0H",
    "outputId": "7d8d53d6-b056-477a-ece9-6732702a6338"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss: 121.692047\n"
     ]
    }
   ],
   "source": [
    "plt.scatter(inputs, outputs, c='b', label = \"data\")\n",
    "plt.scatter(inputs, model(inputs), c='r', label = \"model predictions\")\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "print('Current loss: %1.6f' % loss(model(inputs), outputs, model).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16Ujj6vNYQyX",
    "toc-hr-collapsed": true
   },
   "source": [
    "### Update Weights Based on Gradient\n",
    "\n",
    "> *Assigning blame for bad predictions and delivering justice - repeatedly and a little bit at a time*\n",
    "\n",
    "You should also know that with neural networks it is common to have gradients that are not convex (like what we saw when we applied gradient descent to linear regression). \n",
    "\n",
    "Due to the high complexity of these models and their nonlinearity, it is common for gradient descent to get stuck in a local minimum, but there are ways to combat this:\n",
    "\n",
    "1) Stochastic Gradient Descent\n",
    "\n",
    "2) More advanced Gradient-Descent-based \"Optimizers\" - See Stretch Goals on assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "id": "ZgaGD6YlHoid",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e49d40d84aa2cec0b7a5a207a4cd4a15",
     "grade": false,
     "grade_id": "cell-100d1b1df12abe63",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    " def train(model, inputs, outputs, learning_rate):\n",
    "    with tf.GradientTape() as t: \n",
    "        \n",
    "        \n",
    "    # calculate the loss/error value from our model's predictions\n",
    "        \n",
    "    # calculate the gradient of the loss function wrt to W and wrt B \n",
    "    \n",
    "    # update the value of W using the lr * the rate of change of the loss function wrt W \n",
    "    \n",
    "    # update the value of b using the lr * the rate of change of the loss function wrt b        \n",
    "        current_loss = loss(outputs, model(inputs), model)\n",
    "        \n",
    "        dw, db = t.gradient(current_loss, [model.W, model.b])\n",
    "        \n",
    "        model.W.assign_sub(learning_rate * dw)\n",
    "        \n",
    "        model.b.assign_sub(learning_rate * db)\n",
    "        \n",
    "    # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iziWWURgck8"
   },
   "source": [
    "### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4zn_HgFuHhTr",
    "outputId": "ee89f8a9-798e-428a-b6a9-08f5a364ffea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0: W=8.00 b=40.00 loss=121.69205\n",
      "Epoch  1: W=7.10 b=42.00 loss=78.50096\n",
      "Epoch  2: W=6.38 b=43.60 loss=50.76944\n",
      "Epoch  3: W=5.81 b=44.88 loss=32.96390\n",
      "Epoch  4: W=5.34 b=45.91 loss=21.53146\n",
      "Epoch  5: W=4.97 b=46.73 loss=14.19094\n",
      "Epoch  6: W=4.67 b=47.39 loss=9.47774\n",
      "Epoch  7: W=4.43 b=47.91 loss=6.45148\n",
      "Epoch  8: W=4.23 b=48.34 loss=4.50836\n",
      "Epoch  9: W=4.08 b=48.67 loss=3.26069\n",
      "Epoch 10: W=3.95 b=48.95 loss=2.45957\n",
      "Epoch 11: W=3.85 b=49.16 loss=1.94517\n",
      "Epoch 12: W=3.77 b=49.34 loss=1.61488\n",
      "Epoch 13: W=3.71 b=49.48 loss=1.40279\n",
      "Epoch 14: W=3.66 b=49.59 loss=1.26661\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "# Store Some history of weights\n",
    "Ws, bs = [], []\n",
    "epochs = range(15)\n",
    "\n",
    "for epoch in epochs:\n",
    "    Ws.append(model.W.numpy())\n",
    "    bs.append(model.b.numpy())\n",
    "    y_hat =  model(inputs)\n",
    "    current_loss = loss(outputs, y_hat, model)\n",
    "\n",
    "    train(model, inputs, outputs, learning_rate=0.1)\n",
    "    print('Epoch %2d: W=%1.2f b=%1.2f loss=%2.5f' % (epoch, Ws[-1], bs[-1], current_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "FSEt07wdHvi2",
    "outputId": "c41e84eb-fb5e-4ac3-c08b-e085ebf411f4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, Ws, 'r', epochs, bs, 'b')\n",
    "plt.plot([TRUE_W] * len(epochs), 'r--',\n",
    "         [TRUE_b] * len(epochs), 'b--')\n",
    "plt.legend(['W', 'b', 'True W', 'True b'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "6pKDfpplbUxN",
    "outputId": "826bd434-5c5f-4071-88ca-729ddca6daa9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAArgElEQVR4nO3df3TU9Zno8feTIRESoGJAiyITtutq+REiUqtrFVd+1EWO1rp7W+7AUlwXG9Dl3lO1bLOn6mmxPbW7raLgpq0uNVO1i7XbdauroFyl9q6FLaWC9Xa7BkTtkgSlhEATkuf+8Z1vMky+M/P9zu/JPK9zcpL5ZjLzGaJPPvN8ns/nEVXFGGNM+akq9gCMMcZkxgK4McaUKQvgxhhTpiyAG2NMmbIAbowxZWpUIZ9s4sSJ2tDQUMinNMaYsrdr165OVZ2UeL2gAbyhoYGdO3cW8imNMabsich+r+uWQjHGmDJlAdwYY8qUBXBjjClTBc2Be+nr6+PgwYOcOHGi2EMxBTB69GimTJlCdXV1sYdiTNkregA/ePAg48aNo6GhAREp9nBMHqkqXV1dHDx4kGnTphV7OMaUvaKnUE6cOEF9fb0F7wogItTX19u7LTMiRKPQ0ABVVc7naLTwYyj6DByw4F1B7HdtRoJoFFatgp4e5/b+/c5tgEikcOMo+gzcGGPKTUvLUPB29fQ414fJ41TdAniCu+66i69//etJv//DH/6Qffv2FXBExphSc+CAz+vuVH3/flAdmqrnKIhbAA/IArgxZupUn9cDTdWDK7sAno93I+vXr+f8889nwYIFvPHGGwB861vf4iMf+QizZ8/mhhtuoKenh1deeYUf/ehH3H777TQ1NfGb3/zG837GmJFt/XqorT31Wm2tc/0UvqfqGVLVgn1cdNFFmmjfvn3DriXT1qZaW6vqvBdxPmprneuZ2rlzp86cOVOPHTumR44c0Q996EN67733amdn5+B9Wlpa9P7771dV1RUrVug//dM/DX4v2f1MckF+58aUqrY21XBYVcT57BmHwuFTA5b7EQ4Hei5gp3rE1LKagefj3cjLL7/M9ddfT21tLePHj+faa68F4LXXXuPyyy9n1qxZRKNR9u7d6/nzfu9njBlZIhFob4eBAeezZ/WJ76l6ZsoqgOfr3YhXadtnPvMZHnjgAX75y19y5513Jq1d9ns/Y8wI4yefG4lAayuEwyDifG5tzVmtYVkFcN8LBwFcccUVPPXUUxw/fpyjR4/yL//yLwAcPXqUyZMn09fXRzTuFzNu3DiOHj06eDvZ/Ywx+VHoDTSezxeguiRKhAbaqWKABtqJkrtC8bIK4Pl4NzJnzhw+9alP0dTUxA033MDll18OwJe+9CU++tGPsnDhQi644ILB+3/605/m3nvv5cILL+Q3v/lN0vsZY3Ivz1V5vp+ve62/fG6+xytOfrww5s6dq4kNHV5//XU+/OEP+36MaNT5NzpwwJl5r19f2J1PJntBf+fGuBoanCCYKBx28tCFer5+qqjCI3aKOEnxND8fdLwisktV5yZeL6sZOPhcODDGjEj5rsrzetylRHmTBvqp4k0aWEqUA/jL5+Z7vGUXwI0xlSsf62CQPK9+yxlRvsUqGthPFUoD+/kWq/g/dYt95XPzNV6XBXBjTNnIxzpYqjz1PbRQx6m57jp6uGH0j31Vl+S5itDfRh7gdGAL8CvgdeBS4AzgeeDXsc8T0j1Otht5zMhgv3OTDV8baAJIuddGxPubIgUdL1lu5LkPeFZVLwBmx4L4OmCbqp4HbIvdNsaYvMr1OljSPPcBcpIDyee6XdoALiLjgSuA7wCoaq+qvg9cB2yO3W0z8IncDcsYYwojWZ77ljOiBciBZMfPDPwPgA7gERH5uYh8W0TqgLNU9V2A2OczvX5YRFaJyE4R2dnR0ZGzgZeqhoYGOjs7s75Pttrb25k5cyYAO3fu5K//+q9T3v+ee+455fYf//Ef521sxqRS6I06yfLc99CS952U2fITwEcBc4BNqnohcIwA6RJVbVXVuao6d9KkSRkO07hOnjwZ+Gfmzp3L/fffn/I+iQH8lVdeCfw8xmSrYBt14v5KjO3yKNQGxh6O1fqVcO2ynwB+EDioqv8eu70FJ6D/t4hMBoh9PpSfISbI8Z/n9vZ2LrjgAm666SZmzpxJJBJh69atXHbZZZx33nm8+uqrABw+fJhPfOITNDY2cskll7Bnzx4Aurq6WLRoERdeeCE333yzu+gLQFtbGxdffDFNTU3cfPPN9Pf3pxzL2LFj+dznPsecOXOYP38+7juWK6+8ki984QvMmzeP++67j127djFv3jwuuugiPv7xj/Puu+8CsGvXLmbPns2ll17Kgw8+OPi427dvZ8mSJQB0d3ezcuVKZs2aRWNjI08++STr1q3j+PHjNDU1EYn9xzl27FjAWeS+/fbbmTlzJrNmzeKJJ54YfMwrr7ySP/uzP+OCCy4gEokMvvZ169Yxffp0Ghsbue2227L6/ZjKkufjsx2JfyWSyVWtXz55rWwmfgAvA+fHvr4LuDf2sS52bR3wtXSPk3UVSh7Ok33zzTc1FArpnj17tL+/X+fMmaMrV67UgYEB/eEPf6jXXXedqqrecsstetddd6mq6rZt23T27Nmqqnrrrbfq3XffraqqTz/9tALa0dGh+/bt0yVLlmhvb6+qqjY3N+vmzZtVVTUcDmtHR8ewsQDaFnstd999t65Zs0ZVVefNm6fNzc2qqtrb26uXXnqpHjp0SFVVH3/8cV25cqWqqs6aNUu3b9+uqqq33XabzpgxQ1VVX3zxRb3mmmtUVfWOO+7QtWvXDj7n4cOHVVW1rq7ulLG4t7ds2aILFizQkydP6m9/+1s999xz9Z133tEXX3xRx48fr2+99Zb29/frJZdcoi+//LJ2dXXpH/3RH+nAwICqqr733nvDXqdVoZhkclD0kZxbDuL1BIkf2Z5TnWMkqULx29T4ViAqIjXAfwErcWbv3xeRvwQOAH+eo78pyaX685zF25pp06Yxa9YsAGbMmMH8+fMREWbNmkV7bL/rjh07ePLJJwG46qqr6Orq4siRI7z00kv84Ac/AOCaa65hwoQJAGzbto1du3bxkY98BIDjx49z5pmeywSDqqqq+NSnPgXAsmXL+OQnPzn4Pff6G2+8wWuvvcbChQsB6O/vZ/LkyRw5coT333+fefPmAbB8+XKeeeaZYc+xdetWHn/88cHb7niT2bFjB0uXLiUUCnHWWWcxb948fvaznzF+/HguvvhipkyZAkBTUxPt7e1ccskljB49mptuuolrrrlmcOZvjB9Tp3pvPc96MpzYhTgZkbI6o8NXAFfV3cCwffjA/JyOJp087Us97bTTBr+uqqoavF1VVTWYc1aPt1ruMbRex9GqKitWrOArX/lKxuOKf9y6urrBx50xYwY//elPT7nv+++/76vju6oG6gzv9bpd8f9uoVCIkydPMmrUKF599VW2bdvG448/zgMPPMALL7zg+/lMZVu/fniczUnRh9fkL0F3fZixne1ZPlFhlddOzHzvS03hiiuuGDwudvv27UycOJHx48efcv2ZZ57hvffeA2D+/Pls2bKFQ4ecpYHDhw+z32tqEWdgYIAtW7YA8L3vfY+Pfexjw+5z/vnn09HRMRjA+/r62Lt3L6effjof+MAH2LFjB0DSo20XLVrEAw88MHjbHW91dTV9fX2er/uJJ56gv7+fjo4OXnrpJS6++OKkr6G7u5sjR46wePFivvnNb7J79+6Ur9mYeHkr+kgzyTtGLV+gNEoDgyivAF7Emsy77rqLnTt30tjYyLp169i82SmBv/POO3nppZeYM2cOzz33HFNjf0ymT5/Ol7/8ZRYtWkRjYyMLFy4cXGxMpq6ujr1793LRRRfxwgsv8MUvfnHYfWpqatiyZQuf//znmT17Nk1NTYMVI4888ghr1qzh0ksvZcyYMZ7P8bd/+7e89957zJw5k9mzZ/Piiy8CsGrVKhobGwcXMV3XX389jY2NzJ49m6uuuoqvfe1rfPCDH0z6Go4ePcqSJUtobGxk3rx5fOMb30j5mo1JlI+ij+4zvCd5CrQT5q9o5YHDpZ8yGcYrMZ6vj5xspc/1PtoSkriQOFLZIqYptFvr27SbUwsguqnVpbRl2qayoBgJPTGBkq7JNMb4U+jNOg8cjvBXtNJOmAFkcNb9WKw7TgltrgzEbxWKKYDu7u5iD8GYvEssCHE360D+5mNTp8Jj+yODATteOFw2RSfDlMQMXAvYFcgUl/2uTUE26yRItnzW1lbeb+SLHsBHjx5NV1eX/Y9dAVSVrq4uRo8eXeyhmCLKuho4g/xLiR9pkrGi98Ts6+vj4MGDnDhxomDjMMUzevRopkyZQnV1dbGHYopk4kTo6hp+PW2fyGgU1q4d/sO1tSMjGqeQrCdm0XPg1dXVTJs2rdjDMMYUQDQKR48Ov15dPXwR0W1gfvv+1XyWf6CKATy3oOVgN3a5KnoAN8ZUjpYW6O0dfn38+KH46060F3VF2ceNjKHXO3DHy1dX4xJnAdwYk1fuTPrAgeSH/3V1Oblp11KiPMKNnIZHtPdSDicH5oEFcGNM3vg9Q8q1lCgPcTPjOJZ+1u0q1yLuHCh6FYoxZuTycYbUoA2sJsoyxgcJ3vX1I34BMxUL4MaYvIhGvY+GTbSB1ZwkxBo2+QrcChyljh3NbdDZWbHBGyyAG2PywE2dJBMOO+mSXqpYwyZCySpMEijwHPMZTzcf3xzJ+xb8UmcB3BiTc6lSJ7W1sOPIDKIsoxr1Hbg7qCdCG1ezFcj/7s1yYIuYxpicS1XV93rPOZzT847vPLcCD9LMrWwM9DyVwGbgxlSQILvQszkxMLGqbylR3qSBfoRz8Re8FRhA2JgkeHs9T6WxAG5MhUhsxu6eAugVmIPeNzHQxx8etZQoD7OSBvZTBWmDtwK/o44IbfxBeIDT2zZSXz/8fhVcPTjE65DwfH14NXQwxhRGsobsodDw/ij19d73ra8/tZ9Kc7PTwD2xoXtzs3PfpbRpv58u8LGPAdB2zvZsDD+Ce7mkRZKGDkU/zMoYUxhVVcl3Qrpqa2HFCti0Kbvn+p9E2cRnGUd3oFz3HqbTxN6yPqM7H5IdZmUpFGNGGDelIQKjRjmfGxqGn4ftpacn++DtpExuZLzP4K3ASYQIbVxcs7fsz+guJAvgxowg8blrgP5+5/P+/XDsWP6ffylRvssKX2eYOIE7xIM0U80AjxGht9dKA4OwMkJjRpAgW9dzaQOraeYhqgLUdUdo82xxVumlgUFYADdmBCl08HNn3CH6A+W6n2O+Z/AGKw0MwlIoxpSxxBK+M87I33ONipvuLSXKEcYSZRmjfAZvp64bNtI8uJsykZUGBmMB3Jgy5VWrffSo090mH06edD7vZkagUwPjt8GPq1X2Nm/0XFCt8IMFM2IB3Jgy5ZXv7u11utuEw87tUMj/49XVpb/PISbQyD7f6ZLfj6plbX0bZ0knr4QjtLbCxo3DGwy32cGCGbEAbkwJyGTberJ8d1eX871wGDZvHgrm6aRa/HRPDpzI+75n3d0yltP+sZX7OyM8+qhzffly5/WBkyqZOtUZa0tLsK36JsZrd0++PmwnpjHDtbV572ZMt9Mw2W7JxI/TTlOtqfG9GfKUjw00az+iAxnsphRJ/vqqq4ePyc9rrlQk2YnpawYuIu0i8ksR2S0iO2PX7hKRt2PXdovI4rz+pTFmhPJKhfT0OI19k83Kk3V39/L733s3Ek5lA6sZQFjDpkClgW6FSQNvD1aTeL2+vr7hY7LjYYMLUkb4J6ramXDtG6r69VwOyJhKkyoV0tXlfO0eJgVOnjhZd/dcOEGIGp8NFlxu4HarS0SGqkmClDZaDXgwlgM3psj81j339Dg55Gg0f4GuFwkUvN3SwAcTSgNVhxYkg9R1Ww14MH4DuALPicguEYlvlHSLiOwRkYdFZILXD4rIKhHZKSI7Ozo6sh6wMSNN/NGr6ajCypX+7+/XISYwgDCK9Me9Do4FOMDZhNBh53XHH//q9fqqq6Gm5tRrVgMenN8AfpmqzgH+FFgjIlcAm4APAU3Au8Dfef2gqraq6lxVnTtp0qQcDNmYkSUSGV5W53X+tauvL3fnmri5bre6JEijhQdppoG3Pe9z5MhQzt7r9T3yCDz88KnXktWAZ9NYYqQLfJysiNwFdMfnvkWkAXhaVWem+lk7TtYYf6JRWLYsv89xhFrGcTxwumR5kjNMEoXDzqmC2XA3K8UvgtbWVt6Gn4yPkxWROhEZ534NLAJeE5HJcXe7HngtV4M1ppJFo/mtxtjNDAaQwMG7lypGob6CN+QmT5+sQseqVRx+qlDOAp4SEff+31PVZ0XkURFpwvndtgM352uQxlQKrxlnLgWtMHHfn7uNFoLIxYJksj8CVq3iSBvAVfW/gNke15fnZUTGVLC1a/MTvDewmjU4nRqCnhqY7OCpVGpqcrMgOXXq0NnmideNlREaUzKi0aG671x5lgWDG3L8LlKCk+uO0JZR8K6vdxYoc5Gj9qpgsWqVIRbAjckTP9UT8fdZsSK3z3+EWhaxLVDgdhcql/lcqEwkkttDqbwqWCptATMVa+hgTB4k5rITd1J63cdtf5atZ1nAIrYBwQI3OBtyEmu6g8hHaiMSsYCdjM3AjckDP9UT+Wh/dogJGc26D3A2VR4bclzNzekfx1IbhWcB3JgseaVK/FRP5LKSop1zTtmQ44ebLonQlnRDDjiv67LLUh9La80YisMCuDFZ8OqKs2pV8tZm8SmGXKQblhKlH2Eq72Q06w75qOseGHBe0+LFwxcURZzZuTVjKA4L4MZkIVmqBLzPK+nshIkTnVltthUnbmuzKoIvUqabdSfq6YEf/3j4guKjjzoddkxxWAA3Jo1U1SReNcrgBOfW1uFnmhw75nxPFbq7MxuPO+sO0toMgs26vRw44Myy29udWXl7u826i80CuDEpJEuRuEE8Wc/JqionuI0dm9vxnCCU0axbCT7rTmSbZ0qPBXBjUkhXTZKs9G9gAMaMST5DD8qddQfdBq84pYFVGc66XVZhUpqsDtyYFNJVk9TXJ89lnziRmzG0c87gIqUfbk13J6dzJu8Ffr66OviLv3By3gcOODPv9estXVKKLIAbk0KqsziiUfjd7/L33LuZQSP7gGDpkpNADcGOiY43caItTJYLS6EYk0Kybjnd3c7BU319+Xne3rhFyiDBu5PTswreYCf9lRML4Mak4J7FkVhNEt9wOJfcDTlBW5u5ue5MUiaJbLGyfFgANyYmVbnge9nHxZSy3ZCTaht8skoZL7ZYWV4sgBsDrF7tdHyPLxdctszZsLJsmVNVki9HqM14Q06qvpTgbLbZvPnUdxD19dDW5rzOtjY76a+cBe6JmQ3riWlKUSH6T3rJpMkCOMH7KGP4AKlPwqrE3pEjVcY9MY0Z6dauLfxzHmJC4CYL8bnudMHbZtOVwcoITcXLx2JkMpmWBkL6s7pFnLNJLGhXDgvgxhRIPxJoxg1Ds+6Qj9JAVWeHqAXwymEpFDNi+WlpBsNLBHPNrTDJJF3Syem+grfLargriwVwMyKlO4TKvU9DQ35TKO2cE6jCxA3ce5hOFRq4rttquCuLBXAzIqU7hCo+wOfDsdixU0HPMFGgCqWJvSnvKx4PajXclccCuBmR0h1ClY9+lOCkSwYQxtAXOGWyh+lp0yU1NVBd7byriGctzSqTBXAzIiVLJbjX85ErPsQEoizLKNcdoS3trBtg3Djv81fGjrXgXYksgJuS5HcBMhmvQ6jiUwy5zBU/y4LADYVhaEOO37O6w2E4fNj7e7Z4WZksgJuS42cBMh33ECq3k3oo5KRMVqxw8sfd3U4qIlt9CIvYlrcNOfHWr0//zsJUFgvgpuT4WYBMnJ17XYtEnE7qIkOdc9zPXV3ZHQW7mxkMIIQIHrjdCpNUm3IS1dc7ryfdOwtTWWwjjyk5qRYgo1FYuXIo+O7f78yqq6pOvbZqFfzkJ/DQQ8MX/LJ1Egl08BQE25CTqKYG7rvP+drNc7e0WLccY4dZmRLU0OBd3hcOO6kPv3XboVDynpWZOMQEJvI+kPtt8Km0tVmArnTJDrPyNQMXkXbgKNAPnFTVuSJyBvAE0AC0A/9DVfN8arKpBOvXOzPoxDRKkOANuQ3emc66+4HqLDvkWPA2yQTJgf+JqjbF/RVYB2xT1fOAbbHbxmQtVRecQnM75AQ9q1uB55ifdfAO0ozBVJ5sFjGvAzbHvt4MfCLr0RgTE4k4tc3F4m7IyaRDznGqqUK5mq1Zj2PVqqwfwoxgfgO4As+JyC4Rcf+TOktV3wWIfT7T6wdFZJWI7BSRnR0dHdmP2FSMYtU2LyWa0YacAZwNOXX0Zj2GUAiam607vEnNbxXKZar6joicCTwvIr/y+wSq2gq0grOImcEYTYWaOjV/Z5V4eZYFLGIbEHyR8jnm52TG7Tp5MmcPZUYwXwFcVd+JfT4kIk8BFwP/LSKTVfVdEZkMHMrjOE0F+sM/LFwAz/Ss7lwsUibK9/G2ZuRIm0IRkToRGed+DSwCXgN+BKyI3W0F8M/5GqSpPNEovPBC/p/H3QafyU7KPUzPefCurh6q+TYmHT858LOAHSLyC+BV4F9V9Vngq8BCEfk1sDB225jAvHZRrl2b+w04ifqz2Abv58hXP+rqTu0K/8gjVjZo/LONPKao3HNP8nG0azLuIiUEz3X3UsVoclNgXl1tAdv4k9VGHmPyJV/ncidzglCs1YJ/2WyDTxQKwcCAbYE3uWGHWZmiKlSp4AZWM4AECt5uaeCDNOckeIMTvAcGoL3dgrfJns3ATVHlu1RwA6tZwyYgeIVJJ6cH7kmZjh37anLJZuCmqNav9+7vmAvtnMMaNmV8Vref4B1kt6gd+2pyzQK4KQq38mT58txXm2xgNf0ZNhQ+wNmBzuru7k79/fgKE+tZaXLNUigm76LRU8+vXrwYNm/Oz+JlO+cECtyQ20XKeOGwk+s2Jl9sBm7yyqs92kMP5T54ux1yMpl153KR0mXpElMIFsBNxvw0HvYqE8xHh5xG9mV0+FTQ1mbJ1NUNHf0aCjldgixdYvLNArjJSKrGw/GBPZ8VJkeozfis7gdpZlSOZt01NdDbe2rfzc2bgzVhNiYTthPT+Bafy66q8u54U18Px4/nf3NOvg6fCofh4MH03XzC4aGcfrJOQZYDN7liOzFNVhK3vCcLcPnumnOEWsZxHAi+DX4P09OeX3LgQPoUT2JgrkryPrZY55mbymEpFONLobe8J3I75IzjeMa5bj+HT6mmbmPmtTiZbHOObdox+WYB3PjiZzaZzw052XTICZrrTvbuor7eu5Z7/XonsMezKhRTCBbAjS/JZpOh0NBGlXxsyMm0NHAP0wmhPEbmpSDxr62tDTo7vStL3CbMtmnHFJoF8ArnpxQQvGeZIk5e3D2cKRzO3bhOIhlvg8/VWd1BDp6KRJz72UFVppAsgFewVKWAXhJTJKrOppzVq53bXkE+KHfWnUlp4HPMz+mGHMthm1JnZYQVrKHBu047scrCT9OFsWPh2DE44wzn9uHDTgDs7HSu+9GHECJ4aeAA5Kym21Vba2kQUzqSlRHaDLyCJVuYTLzupwKlu9uZkXd1OXXgjz7qzMhHj04/DrcvZZDgHX/4VK6Dd7LFSmNKjdWBV7BkZ3Enpg6C1jP39Dg9Lf1s6Mm0Q04uW5vFa26GjdnvrDemIGwGXsH8lr9lkgvu6kodvJcSpT/DDjkR2nwH72SbbLzU11vwNuXFAngF81v+tnhx7p5zKVH6EKIsC7xQGbQ0UNWp6W5rS705B5w/XPfd53MwxpQIS6FUuEgkfa73+9/PzXNlc1b3MtoC1XTHlzS6ry9xIVbECfLhsDUYNuXJZuAmrWzPN3Fm3aMy2pDjlgYG3ZCTmAbyerfx6KNOALe6bVOuLIAbIPmGnmyPRN3AaqIsYxT9GZ1fcjVbAz9nfX3yHZO22caMJJZCMcPqvN0NPT/5CXznO5k/btAKkyCnBiZjuWxTSWwGXoESZ9tr1w6vGOnpgX/4B6dRQVDuyYFBg3cvVVltg7f6bVNpLIBXGK/t88ly3AMDwR57KVGOMDbQyYGZlAYmM3asBW9TWSyAlwG/B075ka9zvY9QS5RljOeY78CtwHGqsz410GUNFEylsQBe4oIeOJVOrntUuhty3EYLfihwlDFUodThP0dTV5f6sCw7fMpUGt8BXERCIvJzEXk6dvsuEXlbRHbHPnK43cO4vGbMPT3O9Uyk29Dil1MaWBVoQ4476+7kdD5A8LcBPT1Ojru+fvj3rIGCqURBZuBrgdcTrn1DVZtiHz/O4bhMjN8Dp/xK16zXj6HSQA006z6JUxp4Ju9l9LxTpzo57s5OZ3elNVAwlc5XABeRKcA1wLfzOxyTKNf9FrNtunCICYONFvxyt8HXBDg1MPGdQuIM22q6jfE/A/8mcAdOwUC8W0Rkj4g8LCITvH5QRFaJyE4R2dnR0ZHFUCtTrvstZvpzR6hlAGEi7wfckCM8SLPv0sD6emd2vXmzzbCNSSdtABeRJcAhVd2V8K1NwIeAJuBd4O+8fl5VW1V1rqrOnTRpUpbDrTy57rcY9OcSFymDlAYuo40QA9yK/yP+jh8fGqfNsI1JLW1HHhH5CrAcJ4U5GhgP/EBVl8XdpwF4WlVnpnos68hTGvx2j9/A6kDpklzspIThHYGMqXQZd+RR1b9R1Smq2gB8GnhBVZeJyOS4u10PvJaz0ZqMRaMwcaITpEWcrxNLDr2qOOK1cw4DcU2F/cjFTkqX1XMb4082deBfE5Ffisge4E+A/52jMZkMRaNw442n7qzs6oKVK53Gw+5mIEg+Cz/EhMFTA4ME705OT7mTMhQaSgG1tQ0d4+rF6rmN8SdQAFfV7aq6JPb1clWdpaqNqnqtqr6bnyGadNydmsuWeZ9d0tfndI93NwN1dTmf47nd4IMvUjrb4NOVBg4MDM9n53qB1phKYzsxy0zitvrVq4d2aqaSaqkjPtcdJHgf4Gzf2+C9ZtW5XqA1ptLYcbJlxOvY14ceSh2cU9nAalZnELgV+FZVM2O/u5FwS/o/Hqlm1X46AhljvNkMvIx4bavPJHgvJUovVaxhU+Bt8M8znz8IK2O/u5FIJP2Co82qjckfm4GXkVxUZwQtDYSh1mbXhLbS3w/xa49TpyafgdfXWzmgMflkM/Aykm11RiZ13b8nRIQ2rmbr4Dkq8Scirl8PNTXDfzYUss44xuSbBfAy4lW14YfTaGFc4OB9gLMZzUnPRUr3RMRIBB5++NTa8vp6Zyu8pU2MyS8L4EXit0lD/P1aWmDFimAHUi0lymY+w3i6A22Df5BmGng75X3dlI57QqCq89HZacHbmEJIu5U+l2wrvWP16uHVIzU1MG4cHD7spErcqo34qhNwZuCtrbB8eeoFzKVEuYcWwuz3HbjByXX77QRvW96NKYxkW+ltEbPAolHv0r/e3qEdlG6OecyY5M0cki0eZloa+BZn84fVb9PX5+9nbMONMcVnKZQCa2nxV/rX05O82fCBA07wrK4euuY2FA5aGjgAfKemmZfb3uaRR/ylZ6w00JjSYDPwAstFKaBbjeKeZ7KUKA+zktH4nD7jBO/W2Gacm+ICcSTi5Ny9ZveWMjGmtNgMvMCClALW1XlfX7zYmcn/Xe9q+hhFlGWBgrc763Y34ySyM0qMKQ8WwAssWSmgVwux0aO9H+PHP4bW/QtYwyZG0R8oXdJOmGW08eXJ3sEb7IwSY8qFBfAC8wqOyVqIHT48/OeXEmXP/rEsZFugRco9TCeEMo12HiOSNpVjHXGMKX1WRljCEnPRTk33CqpTnLudyA3eiU0WLJ9tTPnIuCOPyQ8/G3ncdMtSorxJA1GW+Q7eCnRQT4S2YcHb8tnGjAxWhVIEXsfCrlrlfB1JqAi58p4FnL3Pf7oEhg6fit+QEwo56RB3k5ClRIwpfzYDLwKvY2F7epxt8qfMxBcs4JwAwdtZqBRaq5pPCd61tU6O3fLZxowsNgMvgmQLiP39cTNxoug2f8HbPav74Zpmxjy8kZa1QGwTUH29cyqgBW1jRh6bgRdBqlpwd6t899oWX8HbPXhqlCiPX76RVatO3cF5/Hi2ozXGlCoL4EXgVQv+LAsYQBhAeHO/UNeVuk+ZMlTTfSsbUYUXXkh+dooxZuSxAB6A3yNg/Rgzxvm8gdX0IyyK1XWnO4RKAebPZ1p4qKZ78HtJKkJzsX3fGFN6LAfuk9/KEb+Pc11PlIf4LOOSnNMtOME6/nsKvFwznyu2buVAgD+92XbyMcaUJpuB+5SsciRoeqKlBV7pmUGUZb6aLLQTZgChnTA3Vrfx1sNOdUmyoCwJD2g138aMXBbAfUqWhgianmjdv4BG9vkuDbwy3M4oGeDKcDsLHokMzvaTHTj12c/aGSbGVApLofiUrIFC0PSE3zNMFHhn+nza93p/3w3KLS3OHxHboGNM5bEZuE+FPGJVga3MZ/sXUrc2swOnjKlsFsB9KtQRq32MIkIbi9hq5X/GmJQshRJAJJJ9wP49oxnNiWHXFeiknrXcN1gaaOV/xphUbAZeYH/Jt+lP+GdXnN2UZ9J5Sl23lf8ZY1LxHcBFJCQiPxeRp2O3zxCR50Xk17HPE/I3zJHj3+ojLOe7p5QHRmK7KeNZ+Z8xJp0gM/C1wOtxt9cB21T1PGBb7Lbx4TEiTKOdEAPDdlO6rPzPGJOOrwAuIlOAa4Bvx12+Dtgc+3oz8ImcjmyE8mqTligctuBtjEnP7wz8m8AdOIffuc5S1XcBYp/P9PpBEVklIjtFZGdHR0c2Yx0R0uW1LXVijPErbQAXkSXAIVXdlckTqGqrqs5V1bmTJk3K5CFGFK96cnf7u+2cNMYE4WcGfhlwrYi0A48DV4lIG/DfIjIZIPb5UN5GWSpycByhVz35o486JwnaZhxjTBCButKLyJXAbaq6RETuBbpU9asisg44Q1XvSPXzZd2VPvE4QnCm0jZlNsbkWT660n8VWCgivwYWxm6PXLk6jtAYY3Ik0E5MVd0ObI993QXMz/2QSlSujiM0xpgcsZ2YCXasjnJwVAMDUsXBUQ3sWB3LcycrH7HtksaYIrGzUOLsWB3lwk2rqMNJlUzp38+ETavYAXxs/XrvHLjV/BljisRm4HEaWlsGg7erjh4aWlsKdxyhMcb4FKgKJVulXoUyIFVUMfzfYwChSgc8fsIYY/IvH1UoI847Ie98drLrxhhTTBbA47SvWs8xTt0meYxa2ldZntsYU3osgMf52MYIP29u5WDIOer1YCjMz5tb+dhGy3MbY0qP5cCNMabEWQ7cGGNGGAvgxhhTpiyAG2NMmbIAbowxZcoCuDHGlCkL4MYYU6YsgBtjTJmyAG6MMWXKArgxxpQpC+DGGFOmSj+A56ATvDHGjESl3ZEnGuXkjasY1RtrsrB/v3MbrJGCMabilfQMvHtty1DwjhnV20P3WusEb4wxJR3Aa7u8O74nu26MMZWkpAP4Abw74SS7bowxlaSkA/jf13t3yPn7euuQY4wxJR3AP3pfhFuqW2nH6ZDTTphbqlv56H22gGmMMSVdheIUmkS4siXCgQMwdSqsX28FKMYYAyUewMEJ1hawjTFmuJJOoRhjjEnOArgxxpQpC+DGGFOmLIAbY0yZsgBujDFlSlS1cE8mchR4o2BPmJ2JQGexBxFAOY23nMYK5TXechorlNd4iznWsKpOSrxY6DLCN1R1boGfMyMisrNcxgrlNd5yGiuU13jLaaxQXuMtxbFaCsUYY8qUBXBjjClThQ7grQV+vmyU01ihvMZbTmOF8hpvOY0Vymu8JTfWgi5iGmOMyR1LoRhjTJmyAG6MMWWqaAFcRG4TERWRicUaQzoi8iUR2SMiu0XkORE5u9hjSkVE7hWRX8XG/JSInF7sMSUjIn8uIntFZEBESqo0yyUiV4vIGyLynyKyrtjjSUVEHhaRQyLyWrHHko6InCsiL4rI67H/BtYWe0ypiMhoEXlVRH4RG+/dxR6TqygBXETOBRYCpd7c8l5VbVTVJuBp4ItFHk86zwMzVbUR+H/A3xR5PKm8BnwSeKnYA/EiIiHgQeBPgenAUhGZXtxRpfSPwNXFHoRPJ4HPqeqHgUuANSX+b/t74CpVnQ00AVeLyCXFHZKjWDPwbwB3ACW9gqqqv4u7WUfpj/c5VT0Zu/l/gSnFHE8qqvq6qpbyrtyLgf9U1f9S1V7gceC6Io8pKVV9CThc7HH4oarvqup/xL4+CrwOnFPcUSWnju7YzerYR0nEgoIHcBG5FnhbVX9R6OfOhIisF5G3gAilPwOPdyPwTLEHUcbOAd6Ku32QEg4y5UpEGoALgX8v8lBSEpGQiOwGDgHPq2pJjDcvW+lFZCvwQY9vtQBfABbl43kzkWqsqvrPqtoCtIjI3wC3AHcWdIAJ0o03dp8WnLep0UKOLZGfsZYw8bhWErOukUJExgJPAv8r4d1uyVHVfqAptq70lIjMVNWirzfkJYCr6gKv6yIyC5gG/EJEwHmL/x8icrGq/jYfY0kn2Vg9fA/4V4ocwNONV0RWAEuA+VrkIv8A/7al6CBwbtztKcA7RRrLiCMi1TjBO6qqPyj2ePxS1fdFZDvOekPRA3hBUyiq+ktVPVNVG1S1Aed/kjnFCt7piMh5cTevBX5VrLH4ISJXA58HrlXVnmKPp8z9DDhPRKaJSA3waeBHRR7TiCDO7O07wOuq+vfFHk86IjLJregSkTHAAkokFlgdeGpfFZHXRGQPTtqnpMudgAeAccDzsdLHh4o9oGRE5HoROQhcCvyriPxbsccUL7YYfAvwbziLbN9X1b3FHVVyIvIY8FPgfBE5KCJ/WewxpXAZsBy4Kvbf6W4RWVzsQaUwGXgxFgd+hpMDf7rIYwJsK70xxpQtm4EbY0yZsgBujDFlygK4McaUKQvgxhhTpiyAG2NMmbIAbowxZcoCuDHGlKn/D4g2GghXqR5sAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss: 1.179171\n"
     ]
    }
   ],
   "source": [
    "plt.scatter(inputs, outputs, c='b', label = \"data\")\n",
    "plt.scatter(inputs, model(inputs), c='r', label = \"model predictions\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Current loss: %1.6f' % loss(model(inputs), outputs, model).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKUVGoRxgck_"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "In the module project, you will be asked to explain the logic of backpropagation and gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTqZg-6igclA",
    "toc-hr-collapsed": true
   },
   "source": [
    "# Batch Size (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nrm-racgclA"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The What - Stochastic Gradient Descent calculates an approximation of the gradient over the entire dataset by reviewing the predictions of a random sample. \n",
    "\n",
    "The Why - *Speed*. Calculating the gradient over the entire dataset is extremely expensive computationally. \n",
    "\n",
    "### Batch Size\n",
    "Batches are the number of observations our model is shown to make predictions and update the weights. Batches are selected randomly during epoch. All observations are considered when passing thru an epoch at some point.\n",
    "\n",
    "* Smaller Batch = Slower Run Time (but maybe more accurate results)\n",
    "* Default Batch = Balance between speed and accuracy\n",
    "* Large Batch = Very fast, but not nearly as accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNQ2ZCi7I4i6"
   },
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "mZjW2lYVI9Q2",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1b0af5b2611f98d63f8bb6b9427bb93",
     "grade": false,
     "grade_id": "cell-1c90a81f1eece31b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "7a5f9379-453d-459c-d5d0-5194a19070a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "# scale data\n",
    "\n",
    "# YOUR CODE HERE\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "max_pixel_value = 255\n",
    "X_train = X_train / max_pixel_value\n",
    "X_test = X_test / max_pixel_value\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape( (60000, -1) )\n",
    "X_test = X_test.reshape((10000, -1))\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing data helps your model learn\n",
    "\n",
    "Whenever all data is normalized to values within 0 and 1, that ensures that the update to all the weights are updated in equal proportions which can lead to quicker convergence on the optimal weight values. \n",
    "\n",
    "**Hint:** if your dataset's values range accross multiple orders of magnitude (i.e. $10^1,~~10^2,~~10^3,~~10^4$), then gradient descent will update the weights in grossly uneven proportions.  \n",
    "\n",
    "\n",
    "![](https://quicktomaster.com/wp-content/uploads/2020/08/contour_plot.png)\n",
    "\n",
    "There's more to be said about Normalization and Gradient Descent, however there's not enough time! So I highly encourage you to [**read throught this very well written article that explores the impact of normalization on Gradient Descent in much greater detail.**](https://www.jeremyjordan.me/batch-normalization/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "id": "o7x17kDKJSy5",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7d8340907fc4785bc1f1335632de37c",
     "grade": false,
     "grade_id": "cell-38ed3365b403af52",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create a function called create_model that accepts a learing rate for SGD as an input parameter \n",
    "# it should return a complied, 2 hidden layer neural net that uses SGD as the optimizer \n",
    "# Import SGD as discussed here: https://keras.io/api/optimizers/sgd/\n",
    "\n",
    "# create create_model\n",
    "def create_model(lr=.01):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr: float\n",
    "        Learing rate parameter used for Stocastic Gradient Descent \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    model: keras object \n",
    "        A complied keras model \n",
    "    \"\"\"\n",
    "# YOUR CODE HERE\n",
    "    opt = SGD(learning_rate=lr)\n",
    "    model = Sequential([\n",
    "        Dense(32, activation='relu', input_dim=784),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "        \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=opt,\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W-HsAQ-9jgUM",
    "outputId": "a26a7ff3-18f7-49a8-8af3-9f9bc29ee119"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 25,818\n",
      "Trainable params: 25,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "create_model().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF7UE-KluPsX"
   },
   "source": [
    "## Follow Along\n",
    "Let's run a series of experiments for a default, small, and large batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhpDaVFRJl3U"
   },
   "source": [
    "### Default\n",
    "Batch Size is 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-ChVGikgclD",
    "outputId": "d5012af3-4ead-4612-c62d-e58fbf2312ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-23 17:43:30.204427: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:855 train_function  *\n        return step_function(self, iterator)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:838 run_step  **\n        outputs = model.train_step(data)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:796 train_step\n        loss = self.compiled_loss(\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:155 __call__\n        losses = call_fn(y_true, y_pred)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:259 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1643 categorical_crossentropy\n        return backend.categorical_crossentropy(\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4862 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:1161 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (32, 1) and (32, 10) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fy/16bvjs25629769nm3_pgm3cw0000gn/T/ipykernel_43448/1644716826.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# instantiate a model and fit it with batch size of 32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m bt_default = model.fit(X_train, y_train,\n\u001b[0m\u001b[1;32m      4\u001b[0m                       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 763\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    764\u001b[0m             *args, **kwds))\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3279\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:855 train_function  *\n        return step_function(self, iterator)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:838 run_step  **\n        outputs = model.train_step(data)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:796 train_step\n        loss = self.compiled_loss(\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:155 __call__\n        losses = call_fn(y_true, y_pred)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:259 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1643 categorical_crossentropy\n        return backend.categorical_crossentropy(\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4862 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:1161 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (32, 1) and (32, 10) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# instantiate a model and fit it with batch size of 32\n",
    "model = create_model()\n",
    "bt_default = model.fit(X_train, y_train,\n",
    "                      batch_size=32,\n",
    "                      epochs=5,\n",
    "                      validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvsbOFnDJuG0"
   },
   "source": [
    "### Small Batch Size\n",
    "Batch Size is 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "diDzvb-UJ1je",
    "outputId": "caf5ead4-ece1-4f30-e357-6fa0753e0689"
   },
   "outputs": [],
   "source": [
    "# instantiate a model and fit it with batch size of 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iPvvvt5J2Xl"
   },
   "source": [
    "### Large Batch Size\n",
    "Batch Size is 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7h8Z5293KABT",
    "outputId": "4eb4ea99-a5c3-4451-f61c-a71ea4d65042"
   },
   "outputs": [],
   "source": [
    "# instantiate a model and fit it with batch size of 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0ujUz6BKUGz"
   },
   "source": [
    "### Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "o-5DOZNMKYt-",
    "outputId": "05813451-6c69-4ac5-e28d-166dcdd1dd5b"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "batch_sizes = []\n",
    "\n",
    "for exp, result in zip([bt_default, bt_small, bt_large], [\"32_\", \"8_\", \"512_\"]):\n",
    "\n",
    "    df = pd.DataFrame.from_dict(exp.history)\n",
    "    df['epoch'] = df.index.values\n",
    "    df['Batch Size'] = result\n",
    "\n",
    "    batch_sizes.append(df)\n",
    "\n",
    "df = pd.concat(batch_sizes)\n",
    "df['Batch Size'] = df['Batch Size'].astype('str')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "Dlg9uSEEmIJB",
    "outputId": "f5913781-2944-4b3f-c8e3-ad15bd595538"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoch', y='val_accuracy', hue='Batch Size', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "94bJYgz3nkp0",
    "outputId": "8c214962-52ba-4cd9-b714-9e6b4758acb4"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoch', y='val_loss', hue='Batch Size', data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kZ2vUYYgclS"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to experiment with batch size on today's assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46cP9Pm_gclS"
   },
   "source": [
    "# Learning Rate (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bna67ADZgclT",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Overview\n",
    "\n",
    "Learning Rate controls the size of the update to our weights that the optimization algorithm makes. VERY IMPORTANT hyperparameter.\n",
    "\n",
    "* Too high of a learning rate causes unstable results\n",
    "* Too Low of a learning rate the model will underfit\n",
    "* Goldy Locks parameters - it needs be \"just right\"\n",
    "* Scale of 0-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsVYOn7bgcle",
    "toc-hr-collapsed": true
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "Same experiment with Batch but different learning rates:\n",
    "* High Learning = .75\n",
    "* Default Learning = .01\n",
    "* Low Learning Rate = .0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CI_H8Em1NOii"
   },
   "source": [
    "### Default Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Se8cb_ZUNVtL"
   },
   "outputs": [],
   "source": [
    "# instantiate a model and fit it with a learning rate value of 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQZ4SZdKNMRO"
   },
   "source": [
    "### High Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ny72mU_dNWMR"
   },
   "outputs": [],
   "source": [
    "# instantiate a model and fit it with a learning rate value of 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAqDmTVBNSMR"
   },
   "source": [
    "### Low Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ech1ER64NXBn",
    "outputId": "8e359ae7-baf4-4c5d-ea53-4172741e7569"
   },
   "outputs": [],
   "source": [
    "# instantiate a model and fit it with a learning rate value of 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZe6DyhANXdU"
   },
   "source": [
    "### Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "Bn-BdFdMNph-",
    "outputId": "0600ccfa-7a78-4aaa-c1a5-7087c21c31e4"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "batch_sizes = []\n",
    "\n",
    "for exp, result in zip([lr_default, lr_low, lr_high], [\".01_\", \".0001_\", \".75_\"]):\n",
    "\n",
    "    df = pd.DataFrame.from_dict(exp.history)\n",
    "    df['epoch'] = df.index.values\n",
    "    df['Learning Rate'] = result\n",
    "\n",
    "    batch_sizes.append(df)\n",
    "\n",
    "df = pd.concat(batch_sizes)\n",
    "df['Learning Rate'] = df['Learning Rate'].astype('str')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "QgDX7htLpTlQ",
    "outputId": "48537fa8-39a3-4853-ec92-5d8f684df907"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoch', y='val_loss', hue='Learning Rate', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "D8GPXqf_qGs9",
    "outputId": "4a105d15-cd8c-4fb2-a3c9-16ecd7e4084c"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoch', y='val_accuracy', hue='Learning Rate', data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kb2aiw_Sgcl7"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to experiment with different learning rates today.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GszSNVwUrmXy"
   },
   "source": [
    "# Bonus: How do I know if my neural net is overfitting ?\n",
    "\n",
    "Compare train & test losses (or learning metric like accuracy) and look for the gap between the curves. \n",
    "\n",
    "\n",
    "See [**this resource**](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/) for further details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "eP5u3_ZWr3Mn",
    "outputId": "257570fb-8c60-4662-86ab-1b97082d2e02"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoch', y='val_loss', data=df[df['Learning Rate']=='.01_'], label=\"test_loss\")\n",
    "sns.lineplot(x='epoch', y='loss', data=df[df['Learning Rate']=='.01_'], label=\"train_loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "aBt1q68UsHoq",
    "outputId": "52398826-906c-43e3-af57-acd00953f7d0"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='epoch', y='val_accuracy', data=df[df['Learning Rate']=='.01_'])\n",
    "sns.lineplot(x='epoch', y='accuracy', data=df[df['Learning Rate']=='.01_']);"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_422_Train_Lecture.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
